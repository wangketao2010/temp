# # -*- coding: utf-8 -*-
#
# """
#  一些公共模型代码
#  @Time    : 2019/1/30 12:46
#  @Author  : MaCan (ma_cancan@163.com)
#  @File    : models.py
# """
# import os
# import pickle
# import tensorflow as tf
# import numpy as np
# import collections
# from bert_base.bert import modeling
# # from tensorflow.contrib.layers.python.layers import initializers
# import logging
# logger = logging.getLogger(__name__)
# logger.setLevel(level = logging.INFO)
#
# __all__ = ['InputExample', 'InputFeatures', 'create_model', 'convert_id_str',
#            'convert_id_to_label']
#
# class Model(object):
#     def __init__(self, *args, **kwargs):
#         pass
#
#
# class InputExample(object):
#     """A single training/test example for simple sequence classification."""
#
#     def __init__(self, guid=None, text=None, label=None):
#         """Constructs a InputExample.
#         Args:
#           guid: Unique id for the example.
#           text_a: string. The untokenized text of the first sequence. For single
#             sequence tasks, only this sequence must be specified.
#           label: (Optional) string. The label of the example. This should be
#             specified for train and dev examples, but not for test examples.
#         """
#         self.guid = guid
#         self.text = text
#         self.label = label
#
# class InputFeatures(object):
#     """A single set of features of data."""
#
#     def __init__(self, input_ids, input_mask, segment_ids, label_ids, ):
#         self.input_ids = input_ids
#         self.input_mask = input_mask
#         self.segment_ids = segment_ids
#         self.label_ids = label_ids
#         # self.label_mask = label_mask
#
# def write_tokens(tokens, output_dir, mode):
#     """
#     将序列解析结果写入到文件中
#     只在mode=test的时候启用
#     :param tokens:
#     :param mode:
#     :return:
#     """
#     if mode == "test":
#         path = os.path.join(output_dir, "token_" + mode + ".txt")
#         wf = open(path, 'a', encoding='utf-8')
#         for token in tokens:
#             if token != "**NULL**":
#                 wf.write(token + '\n')
#         wf.close()
#
# class DataProcessor(object):
#     """Base class for data converters for sequence classification data sets."""
#
#     def get_train_examples(self, data_dir):
#         """Gets a collection of `InputExample`s for the train set."""
#         raise NotImplementedError()
#
#     def get_dev_examples(self, data_dir):
#         """Gets a collection of `InputExample`s for the dev set."""
#         raise NotImplementedError()
#
#     def get_labels(self):
#         """Gets the list of labels for this data set."""
#         raise NotImplementedError()
#
#     @classmethod
#     def _read_data(cls, input_file):
#         """Reads a BIO data."""
#         with open(input_file, 'r', encoding='utf-8') as f:
#             lines = []
#             words = []
#             labels = []
#             for line in f:
#                 contends = line.strip()
#                 tokens = contends.split(' ')
#                 if len(tokens) == 2:
#                     words.append(tokens[0])
#                     labels.append(tokens[1])
#                 else:
#                     if len(contends) == 0:
#                         l = ' '.join([label for label in labels if len(label) > 0])
#                         w = ' '.join([word for word in words if len(word) > 0])
#                         lines.append([l, w])
#                         words = []
#                         labels = []
#                         continue
#                 if contends.startswith("-DOCSTART-"):
#                     words.append('')
#                     continue
#             return lines
#
#
# def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode):
#     """
#     将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中
#     :param ex_index: index
#     :param example: 一个样本
#     :param label_list: 标签列表
#     :param max_seq_length:
#     :param tokenizer:
#     :param output_dir
#     :param mode:
#     :return:
#     """
#     label_map = {}
#     # 1表示从1开始对label进行index化
#     for (i, label) in enumerate(label_list, 1):
#         label_map[label] = i
#     # 保存label->index 的map
#     if not os.path.exists(os.path.join(output_dir, 'label2id.pkl')):
#         with open(os.path.join(output_dir, 'label2id.pkl'), 'wb', encoding='utf-8') as w:
#             pickle.dump(label_map, w)
#
#     textlist = example.text.split(' ')
#     labellist = example.label.split(' ')
#     tokens = []
#     labels = []
#     for i, word in enumerate(textlist):
#         # 分词，如果是中文，就是分字,但是对于一些不在BERT的vocab.txt中得字符会被进行WordPice处理（例如中文的引号），可以将所有的分字操作替换为list(input)
#         token = tokenizer.tokenize(word)
#         tokens.extend(token)
#         label_1 = labellist[i]
#         for m in range(len(token)):
#             if m == 0:
#                 labels.append(label_1)
#             else:  # 一般不会出现else
#                 labels.append("X")
#     # tokens = tokenizer.tokenize(example.text)
#     # 序列截断
#     if len(tokens) >= max_seq_length - 1:
#         tokens = tokens[0:(max_seq_length - 2)]  # -2 的原因是因为序列需要加一个句首和句尾标志
#         labels = labels[0:(max_seq_length - 2)]
#     ntokens = []
#     segment_ids = []
#     label_ids = []
#     ntokens.append("[CLS]")  # 句子开始设置CLS 标志
#     segment_ids.append(0)
#     # append("O") or append("[CLS]") not sure!
#     label_ids.append(label_map["[CLS]"])  # O OR CLS 没有任何影响，不过我觉得O 会减少标签个数,不过拒收和句尾使用不同的标志来标注，使用LCS 也没毛病
#     for i, token in enumerate(tokens):
#         ntokens.append(token)
#         segment_ids.append(0)
#         label_ids.append(label_map[labels[i]])
#     ntokens.append("[SEP]")  # 句尾添加[SEP] 标志
#     segment_ids.append(0)
#     # append("O") or append("[SEP]") not sure!
#     label_ids.append(label_map["[SEP]"])
#     input_ids = tokenizer.convert_tokens_to_ids(ntokens)  # 将序列中的字(ntokens)转化为ID形式
#     input_mask = [1] * len(input_ids)
#     # label_mask = [1] * len(input_ids)
#     # padding, 使用
#     while len(input_ids) < max_seq_length:
#         input_ids.append(0)
#         input_mask.append(0)
#         segment_ids.append(0)
#         # we don't concerned about it!
#         label_ids.append(0)
#         ntokens.append("**NULL**")
#         # label_mask.append(0)
#     # print(len(input_ids))
#     assert len(input_ids) == max_seq_length
#     assert len(input_mask) == max_seq_length
#     assert len(segment_ids) == max_seq_length
#     assert len(label_ids) == max_seq_length
#     # assert len(label_mask) == max_seq_length
#     # 结构化为一个类
#     feature = InputFeatures(
#         input_ids=input_ids,
#         input_mask=input_mask,
#         segment_ids=segment_ids,
#         label_ids=label_ids,
#         # label_mask = label_mask
#     )
#     # mode='test'的时候才有效
#     write_tokens(ntokens, output_dir, mode)
#     return feature
#
#
# def filed_based_convert_examples_to_features(
#         examples, label_list, max_seq_length, tokenizer, output_file, output_dir, mode=None):
#     """
#     将数据转化为TF_Record 结构，作为模型数据输入
#     :param examples:  样本
#     :param label_list:标签list
#     :param max_seq_length: 预先设定的最大序列长度
#     :param tokenizer: tokenizer 对象
#     :param output_file: tf.record 输出路径
#     :param mode:
#     :return:
#     """
#     writer = tf.python_io.TFRecordWriter(output_file)
#     # 遍历训练数据
#     for (ex_index, example) in enumerate(examples):
#         if ex_index % 5000 == 0:
#             logger.info("Writing example %d of %d" % (ex_index, len(examples)))
#         # 对于每一个训练样本,
#         feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode)
#
#         def create_int_feature(values):
#             f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
#             return f
#
#         features = collections.OrderedDict()
#         features["input_ids"] = create_int_feature(feature.input_ids)
#         features["input_mask"] = create_int_feature(feature.input_mask)
#         features["segment_ids"] = create_int_feature(feature.segment_ids)
#         features["label_ids"] = create_int_feature(feature.label_ids)
#         # features["label_mask"] = create_int_feature(feature.label_mask)
#         # tf.train.Example/Feature 是一种协议，方便序列化？？？
#         tf_example = tf.train.Example(features=tf.train.Features(feature=features))
#         writer.write(tf_example.SerializeToString())
#
#
# def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):
#     name_to_features = {
#         "input_ids": tf.FixedLenFeature([seq_length], tf.int64),
#         "input_mask": tf.FixedLenFeature([seq_length], tf.int64),
#         "segment_ids": tf.FixedLenFeature([seq_length], tf.int64),
#         "label_ids": tf.FixedLenFeature([seq_length], tf.int64),
#         # "label_ids":tf.VarLenFeature(tf.int64),
#         # "label_mask": tf.FixedLenFeature([seq_length], tf.int64),
#     }
#
#     def _decode_record(record, name_to_features):
#         example = tf.parse_single_example(record, name_to_features)
#         for name in list(example.keys()):
#             t = example[name]
#             if t.dtype == tf.int64:
#                 t = tf.to_int32(t)
#             example[name] = t
#         return example
#
#     def input_fn(params):
#         batch_size = params["batch_size"]
#         d = tf.data.TFRecordDataset(input_file)
#         if is_training:
#             d = d.repeat()
#             d = d.shuffle(buffer_size=300)
#         d = d.apply(tf.data.experimental.map_and_batch(lambda record: _decode_record(record, name_to_features),
#                                                        batch_size=batch_size,
#                                                        num_parallel_calls=8,  # 并行处理数据的CPU核心数量，不要大于你机器的核心数
#                                                        drop_remainder=drop_remainder))
#         d = d.prefetch(buffer_size=4)
#         return d
#
#     return input_fn
#
#
# def create_model(bert_config, is_training, input_ids, input_mask,
#                  segment_ids, use_one_hot_embeddings,):
#     """
#     创建X模型
#     :param bert_config: bert 配置
#     :param is_training:
#     :param input_ids: 数据的idx 表示
#     :param input_mask:
#     :param segment_ids:
#     :param use_one_hot_embeddings:
#     :return:
#     """
#     # 使用数据加载BertModel,获取对应的字embedding
#     model = modeling.BertModel(
#         config=bert_config,
#         is_training=is_training,
#         input_ids=input_ids,
#         input_mask=input_mask,
#         token_type_ids=segment_ids,
#         use_one_hot_embeddings=use_one_hot_embeddings
#     )
#     # 获取对应的embedding 输入数据[batch_size, seq_length, embedding_size]
#     embedding = model.get_sequence_output() # (batch_size, seq_length, embedding_size)
#     print('bert最有一个隐层的输出维度：', embedding.shape)
#     return embedding
#
# # def decode_labels(labels, batch_size):
# #     new_labels = []
# #     for row in range(batch_size):
# #         label = []
# #         for i in labels[row]:
# #             i = i.decode('utf-8')
# #             if i == '**PAD**':
# #                 break
# #             if i in ['[CLS]', '[SEP]']:
# #                 continue
# #             label.append(i)
# #         new_labels.append(label)
# #     return new_labels
#
#
# def convert_id_str(input_ids, batch_size):
#     res = []
#     for row in range(batch_size):
#         line = []
#         for i in input_ids[row]:
#             i = i.decode('utf-8')
#             if i == '**PAD**':
#                 break
#             if i in ['[CLS]', '[SEP]']:
#                 continue
#
#             line.append(i)
#         res.append(line)
#     return res
#
#
# def convert_id_to_label(pred_ids_result, idx2label, batch_size):
#     """
#     将id形式的结果转化为真实序列结果
#     :param pred_ids_result:
#     :param idx2label:
#     :return:
#     """
#     result = []
#     index_result = []
#     for row in range(batch_size):
#         curr_seq = []
#         curr_idx = []
#         ids = pred_ids_result[row]
#         for idx, id in enumerate(ids):
#             if id == 0:
#                 break
#             curr_label = idx2label[id]
#             if curr_label in ['[CLS]', '[SEP]']:
#                 if id == 102 and (idx < len(ids) and ids[idx + 1] == 0):
#                     break
#                 continue
#             # elif curr_label == '[SEP]':
#             #     break
#             curr_seq.append(curr_label)
#             curr_idx.append(id)
#         result.append(curr_seq)
#         index_result.append(curr_idx)
#     return result, index_result
#
#
# def get_embedding(x, seq_len_ls, max_len, batch_size):
#     embedding_matrix = np.zeros((batch_size,max_len, 768))
#     for ind, batch_one in enumerate(x):
#         embedding = np.zeros((max_len, 768))
#         len_sen = seq_len_ls[ind]
#         embedding[:len_sen:,] = bc.encode([x for x in batch_one[:len_sen]])
#         embedding_matrix[ind] = embedding
#     embedding_matrix = tf.convert_to_tensor(embedding_matrix)
#     return embedding_matrix
#
#
# if __name__ == '__main__':
#     from bert_serving.client import BertClient
#
#     bc = BertClient()
#     m = bc.encode(['First do it', 'then do it right', 'then do it better', '0'])
#     print(m.shape)
#     x = [['First', 'do', 'it'], ['then', 'do', 'it', 'right'],['then', 'do', 'it', 'better']]
#     embedding = get_embedding(x, [3,4,4],10, 3)
#     print(embedding.shape)
#     # print(m[1][0])
#     # print(m[1][1])
#     # print(m[1][3])
#     # print(m[1][10])
#     # print(m[3][1])
